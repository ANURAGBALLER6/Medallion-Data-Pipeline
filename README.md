# Medallion Data Pipeline

A comprehensive **ETL pipeline** implementing the **Medallion Architecture** (Bronze, Silver, Gold layers).  
The pipeline processes mobility_dataset from **Google Sheets** into **PostgreSQL**, with robust data validation, quality checks, and audit logging.

---

## ğŸ›ï¸ What is the Medallion Architecture?

The **Medallion Architecture** is a **data design pattern** for building scalable, reliable, and high-quality data pipelines.  
It organizes data into **progressive layers** â€” Bronze, Silver, and Gold â€” ensuring that each step improves **data quality** and **business value**.

- **Bronze Layer (Raw):** Stores raw ingested data with minimal transformations.  
- **Silver Layer (Clean):** Cleansed, validated, and standardized data with enforced quality checks.  
- **Gold Layer (Analytics):** Curated, aggregated datasets ready for reporting, dashboards, and business KPIs.  

---

## ğŸ—ï¸ Architecture Overview

- ğŸ¥‰ **Bronze Layer**  
  Raw data ingestion from **Google Sheets**   
- âœ… COMPLETE  

- ğŸ¥ˆ **Silver Layer**  
  Cleaned, validated, and transformed data  
  âœ… COMPLETE  

- ğŸ¥‡ **Gold Layer**  
  Business analytics and KPIs  
  ğŸš§ READY FOR DEVELOPMENT  

---

## ğŸ”„ Dataflow

```mermaid
flowchart TD
    GS[Google Sheets] --> B[ğŸ¥‰ Bronze Layer: Raw Storage]
    B --> S[ğŸ¥ˆ Silver Layer: Cleaned & Validated]
    S --> G[ğŸ¥‡ Gold Layer: Analytics & KPIs]

    B --> RS[Raw Storage]
    S --> QC[Data Quality Checks]
    S --> AL[Audit Logging]
    S --> VR[Validation Rules]

```
## ğŸ“‚ Project Structure
```aiignore
Medallion-Data-Pipeline/
â”‚
â”œâ”€â”€ bronze/                     # Raw data ingestion layer
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ data_loader.py      # Raw data loading scripts
â”‚       â””â”€â”€ database_setup.py   # Database initialization
â”‚
â”œâ”€â”€ gold/                       # Business-ready data layer
â”‚   â””â”€â”€ README.md               # Gold layer documentation
â”‚
â”œâ”€â”€ logs/                       # Application logging
â”‚   â”œâ”€â”€ data_loader.log         # Data loading logs
â”‚   â”œâ”€â”€ database_setup.log      # Database setup logs
â”‚   â”œâ”€â”€ etl.log                 # ETL process logs
â”‚   â”œâ”€â”€ silver.log              # Silver layer logs
â”‚   â””â”€â”€ silver_builder.log      # Silver builder logs
â”‚
â”œâ”€â”€ silver/                     # Cleaned and transformed data layer
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ silver_builder.py   # Silver layer processing
â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â”œâ”€â”€ etl.py                  # ETL operations
â”‚   â””â”€â”€ README.md               # Silver layer documentation
â”‚
â”œâ”€â”€ External Libraries/         # Third-party dependencies
â”œâ”€â”€ Scratches and Consoles      # Development workspace
â””â”€â”€ requirements.txt            # Project dependencies

```
---

## âš™ï¸ Prerequisites (Ubuntu Setup)

Follow these steps to prepare your Ubuntu environment for the Medallion Data Pipeline.

### 1ï¸âƒ£ Install PyCharm (Community Edition)

```bash
  sudo snap install pycharm-community --classic
```

## 2ï¸âƒ£ Install PostgreSQL

```bash
    sudo apt update
    sudo apt install postgresql postgresql-contrib -
```

## 3ï¸âƒ£ Configure PostgreSQL

```bash
    ##Switch to the PostgreSQL user:
    sudo -i -u postgres
```

```bash
    ##Create a new database and user (replace myuser and mypassword with your own):
    psql
    CREATE DATABASE medallion_db;
    CREATE USER myuser WITH ENCRYPTED PASSWORD 'mypassword';
    GRANT ALL PRIVILEGES ON DATABASE medallion_db TO myuser;
    \q
    exit
```

## 3ï¸âƒ£ Install DBeaver (PostgreSQL GUI Client)

```bash
    sudo apt update
    sudo apt install dbeaver-ce -y
```
## âš™ï¸ Project Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/Medallion-Data-Pipeline.git
   cd Medallion-Data-Pipeline
##

2. **Create and activate a virtual environment:**
    ```bash
   git clone https://github.com/your-username/Medallion-Data-Pipeline.git
   cd Medallion-Data-Pipeline
##

3. **Install dependencies:**
    ```bash
   pip install -r requirements.txt
##

4. **Configure database connection in silver/config.py using the credentials created above.**
##

5. **Run the pipeline:**
    ```bash
   python silver/etl.py
##
6. **Check logs:
Logs will be available inside the logs/ directory.**
##


















